{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e4bb0ea",
   "metadata": {},
   "source": [
    "### feature_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad075b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import defaultdict\n",
    "from datetime import date\n",
    "from tqdm import tqdm\n",
    "from itertools import product\n",
    "\n",
    "# distant_df = pd.read_csv('../data/us_air_distance.csv')\n",
    "# distant_map = distant_df.set_index(['source_origin','target_origin']).to_dict()['distance']\n",
    "# distant_map.update(distant_df.set_index(['target_origin','source_origin']).to_dict()['distance'])\n",
    "# population_map = pd.read_csv('../data/us_air_population_all.csv').set_index('Unnamed: 0').fillna(0).to_dict()['0']\n",
    "\n",
    "def get_gravitation(edges):\n",
    "    def my_divid(a,b):\n",
    "        if b==0 or a==0:\n",
    "            return None\n",
    "        else:\n",
    "            return a/b\n",
    "    Gra = []\n",
    "    for e in edges:\n",
    "        u, v = e\n",
    "        d = distant_map.get(e , 0)\n",
    "        ni = population_map.get(u, 0)\n",
    "        nj = population_map.get(v, 0)\n",
    "        Gra.append(my_divid(ni*nj, d**2))\n",
    "    meanv = np.mean([i for i in Gra if i])\n",
    "    return [i if i else meanv for i in Gra]\n",
    "\n",
    "def features_extractor(graphs, dates):\n",
    "    def local_path(G, nodeList, epsilon = 0.01):\n",
    "        A = nx.adjacency_matrix(G, nodelist=nodeList, weight = None).todense()\n",
    "        return (A**2+epsilon*A**3)\n",
    "\n",
    "    def l3_path(G, nodeList):\n",
    "        A = nx.adjacency_matrix(G, nodelist=nodeList, weight = None).todense()\n",
    "        return (A**3)\n",
    "\n",
    "    def weighted_local_path(G, nodeList, epsilon = 0.01):\n",
    "        A = nx.adjacency_matrix(G, nodelist=nodeList, weight='weight').todense()\n",
    "        return (A**2+epsilon*A**3)\n",
    "\n",
    "    X = defaultdict(list)\n",
    "    for i in tqdm(range(len(graphs)-1)):\n",
    "        G, H = graphs[i], graphs[i+1]\n",
    "        G.add_nodes_from([n for n in H if n not in G])\n",
    "        H.add_nodes_from([n for n in G if n not in H])\n",
    "        Hedges = set(H.edges())\n",
    "        Gedges = list(G.edges())\n",
    "        nodeList = list(G.nodes())\n",
    "        nodeIndex = {node: idx for idx,node in enumerate(nodeList)}\n",
    "        year = dates[i]\n",
    "\n",
    "        Ki = dict(G.degree())\n",
    "        Wi = dict(G.degree(weight='weight'))\n",
    "        LPI = local_path(G, nodeList)\n",
    "        L3 = l3_path(G, nodeList)\n",
    "        WLPI = weighted_local_path(G, nodeList)\n",
    "        Gra = get_gravitation(Gedges)\n",
    "\n",
    "        added_edges = list(nx.difference(H,G).edges())\n",
    "\n",
    "        for j, e in enumerate(Gedges):\n",
    "            u, v = e\n",
    "            common_ns = list(nx.common_neighbors(G,u,v))\n",
    "            w_common_ns = sum([min(G[u][z]['weight'], G[v][z]['weight']) for z in common_ns])\n",
    "            union_ns = set(G.neighbors(u))|set(G.neighbors(v))\n",
    "            w_union_ns = Wi[u] + Wi[v]- w_common_ns\n",
    "            if(w_union_ns==0): print(Wi[u] , Wi[v], [min(G[u][z]['weight'], G[v][z]['weight']) for z in common_ns])\n",
    "            X['Edge'].append(e)\n",
    "            X['Year'].append(year)\n",
    "\n",
    "            X['Common Neighbor'].append(len(common_ns))\n",
    "            X['Weighted Common Neighbor'].append(w_common_ns)\n",
    "\n",
    "            X['Salton'].append(len(common_ns)/math.sqrt(Ki[u]*Ki[v]))\n",
    "            X['Weighted Salton'].append(w_common_ns/math.sqrt(Wi[u]*Wi[v]))\n",
    "\n",
    "            X['Sorensen'].append(2*len(common_ns)/(Ki[u]+Ki[v]))\n",
    "            X['Weighted Sorensen'].append(2*w_common_ns/(Wi[u]+Wi[v]))\n",
    "\n",
    "            X['Hub Promoted'].append(len(common_ns)/min(Ki[u],Ki[v]))\n",
    "            X['Weighted Hub Promoted'].append(w_common_ns/min(Wi[u],Wi[v]))\n",
    "\n",
    "            X['Hub Depressed'].append(len(common_ns)/max(Ki[u],Ki[v]))\n",
    "            X['Weighted Hub Depressed'].append(w_common_ns/max(Wi[u],Wi[v]))\n",
    "\n",
    "            X['Leicht Holme Newman'].append(len(common_ns)/(Ki[u]*Ki[v]))\n",
    "            X['Weighted Leicht Holme Newman'].append(w_common_ns/(Wi[u]*Wi[v]))\n",
    "\n",
    "            X['Preferential Attachment'].append(Ki[u]*Ki[v])\n",
    "            X['Weighted Preferential Attachment'].append(Wi[u]*Wi[v])\n",
    "\n",
    "            X['Local Path'].append(LPI[nodeIndex[u],nodeIndex[v]])\n",
    "            X['L3 Path'].append(L3[nodeIndex[u],nodeIndex[v]])\n",
    "            X['Weighted Local Path'].append(WLPI[nodeIndex[u],nodeIndex[v]])\n",
    "            if len(common_ns)>0:\n",
    "                X['Resource Allocation'].append(sum([1/Ki[z] for z in common_ns]))\n",
    "                X['Weighted Resource Allocation'].append(w_common_ns*sum([1/Wi[z] for z in common_ns]))\n",
    "\n",
    "                X['Adamic Adar'].append(sum([1/math.log(Ki[z]) for z in common_ns]))\n",
    "                X['Weighted Adamic Adar'].append(w_common_ns*sum([1/math.log(Wi[z]+1) for z in common_ns]))\n",
    "\n",
    "                X['Jaccard'].append(len(common_ns)/len(union_ns))\n",
    "                X['Weighted Jaccard'].append(w_common_ns/w_union_ns)\n",
    "            else:\n",
    "                X['Resource Allocation'].append(0)\n",
    "                X['Weighted Resource Allocation'].append(0)\n",
    "                X['Adamic Adar'].append(0)\n",
    "                X['Weighted Adamic Adar'].append(0)\n",
    "                X['Jaccard'].append(0)\n",
    "                X['Weighted Jaccard'].append(0)\n",
    "\n",
    "            X['Removed'].append(e not in Hedges)\n",
    "            X['Gravity'].append(Gra[j])\n",
    "            X['Curr Weight'].append(G[u][v]['weight'])\n",
    "            X['Next Weight'].append(H[u][v]['weight'] if e in Hedges else 0)\n",
    "\n",
    "            X['Curr FWeight'].append(G[u][v]['weight']/G.size(weight='weight'))\n",
    "            X['Next FWeight'].append(H[u][v]['weight']/H.size(weight='weight') if e in Hedges else 0)\n",
    "\n",
    "    df = pd.DataFrame(X)\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5edb7ba1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/raw_usair_data/2004_T_T100D_SEGMENT_ALL_CARRIER.csv',\n",
       " 'data/raw_usair_data/2005_T_T100D_SEGMENT_ALL_CARRIER.csv',\n",
       " 'data/raw_usair_data/2006_T_T100D_SEGMENT_ALL_CARRIER.csv',\n",
       " 'data/raw_usair_data/2007_T_T100D_SEGMENT_ALL_CARRIER.csv',\n",
       " 'data/raw_usair_data/2008_T_T100D_SEGMENT_ALL_CARRIER.csv',\n",
       " 'data/raw_usair_data/2009_T_T100D_SEGMENT_ALL_CARRIER.csv',\n",
       " 'data/raw_usair_data/2010_T_T100D_SEGMENT_ALL_CARRIER.csv',\n",
       " 'data/raw_usair_data/2011_T_T100D_SEGMENT_ALL_CARRIER.csv',\n",
       " 'data/raw_usair_data/2012_T_T100D_SEGMENT_ALL_CARRIER.csv',\n",
       " 'data/raw_usair_data/2013_T_T100D_SEGMENT_ALL_CARRIER.csv',\n",
       " 'data/raw_usair_data/2014_T_T100D_SEGMENT_ALL_CARRIER.csv',\n",
       " 'data/raw_usair_data/2015_T_T100D_SEGMENT_ALL_CARRIER.csv',\n",
       " 'data/raw_usair_data/2016_T_T100D_SEGMENT_ALL_CARRIER.csv',\n",
       " 'data/raw_usair_data/2017_T_T100D_SEGMENT_ALL_CARRIER.csv',\n",
       " 'data/raw_usair_data/2018_T_T100D_SEGMENT_ALL_CARRIER.csv',\n",
       " 'data/raw_usair_data/2019_T_T100D_SEGMENT_ALL_CARRIER.csv',\n",
       " 'data/raw_usair_data/2020_T_T100D_SEGMENT_ALL_CARRIER.csv',\n",
       " 'data/raw_usair_data/2021_T_T100D_SEGMENT_ALL_CARRIER.csv',\n",
       " 'data/raw_usair_data/2022_T_T100D_SEGMENT_ALL_CARRIER.csv']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from glob import glob\n",
    "from_file = 'data/raw_usair_data/*.csv'\n",
    "glob(from_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85502aab",
   "metadata": {},
   "source": [
    "### raw2features_usair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3538241f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                | 0/19 [00:00<?, ?it/s]/tmp/ipykernel_443/2200619574.py:66: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  df=pd.read_csv(f,engine=\"python\",error_bad_lines=False)\n",
      "  5%|████▋                                                                                   | 1/19 [00:12<03:51, 12.84s/it]/tmp/ipykernel_443/2200619574.py:66: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  df=pd.read_csv(f,engine=\"python\",error_bad_lines=False)\n",
      " 11%|█████████▎                                                                              | 2/19 [00:25<03:37, 12.80s/it]/tmp/ipykernel_443/2200619574.py:66: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  df=pd.read_csv(f,engine=\"python\",error_bad_lines=False)\n",
      " 16%|█████████████▉                                                                          | 3/19 [00:38<03:28, 13.03s/it]/tmp/ipykernel_443/2200619574.py:66: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  df=pd.read_csv(f,engine=\"python\",error_bad_lines=False)\n",
      " 21%|██████████████████▌                                                                     | 4/19 [00:52<03:19, 13.31s/it]/tmp/ipykernel_443/2200619574.py:66: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  df=pd.read_csv(f,engine=\"python\",error_bad_lines=False)\n",
      " 26%|███████████████████████▏                                                                | 5/19 [01:05<03:04, 13.21s/it]/tmp/ipykernel_443/2200619574.py:66: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  df=pd.read_csv(f,engine=\"python\",error_bad_lines=False)\n",
      " 32%|███████████████████████████▊                                                            | 6/19 [01:17<02:47, 12.90s/it]/tmp/ipykernel_443/2200619574.py:66: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  df=pd.read_csv(f,engine=\"python\",error_bad_lines=False)\n",
      " 37%|████████████████████████████████▍                                                       | 7/19 [01:30<02:34, 12.88s/it]/tmp/ipykernel_443/2200619574.py:66: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  df=pd.read_csv(f,engine=\"python\",error_bad_lines=False)\n",
      " 42%|█████████████████████████████████████                                                   | 8/19 [01:45<02:27, 13.40s/it]/tmp/ipykernel_443/2200619574.py:66: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  df=pd.read_csv(f,engine=\"python\",error_bad_lines=False)\n",
      " 47%|█████████████████████████████████████████▋                                              | 9/19 [01:59<02:16, 13.64s/it]/tmp/ipykernel_443/2200619574.py:66: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  df=pd.read_csv(f,engine=\"python\",error_bad_lines=False)\n",
      " 53%|█████████████████████████████████████████████▊                                         | 10/19 [02:13<02:04, 13.78s/it]/tmp/ipykernel_443/2200619574.py:66: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  df=pd.read_csv(f,engine=\"python\",error_bad_lines=False)\n",
      " 58%|██████████████████████████████████████████████████▎                                    | 11/19 [02:28<01:53, 14.16s/it]/tmp/ipykernel_443/2200619574.py:66: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  df=pd.read_csv(f,engine=\"python\",error_bad_lines=False)\n",
      " 63%|██████████████████████████████████████████████████████▉                                | 12/19 [02:44<01:42, 14.61s/it]/tmp/ipykernel_443/2200619574.py:66: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  df=pd.read_csv(f,engine=\"python\",error_bad_lines=False)\n",
      " 68%|███████████████████████████████████████████████████████████▌                           | 13/19 [03:00<01:31, 15.23s/it]/tmp/ipykernel_443/2200619574.py:66: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  df=pd.read_csv(f,engine=\"python\",error_bad_lines=False)\n",
      " 74%|████████████████████████████████████████████████████████████████                       | 14/19 [03:17<01:18, 15.64s/it]/tmp/ipykernel_443/2200619574.py:66: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  df=pd.read_csv(f,engine=\"python\",error_bad_lines=False)\n",
      " 79%|████████████████████████████████████████████████████████████████████▋                  | 15/19 [03:33<01:03, 15.82s/it]/tmp/ipykernel_443/2200619574.py:66: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  df=pd.read_csv(f,engine=\"python\",error_bad_lines=False)\n",
      " 84%|█████████████████████████████████████████████████████████████████████████▎             | 16/19 [03:50<00:48, 16.20s/it]/tmp/ipykernel_443/2200619574.py:66: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  df=pd.read_csv(f,engine=\"python\",error_bad_lines=False)\n",
      " 89%|█████████████████████████████████████████████████████████████████████████████▊         | 17/19 [04:03<00:30, 15.23s/it]/tmp/ipykernel_443/2200619574.py:66: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  df=pd.read_csv(f,engine=\"python\",error_bad_lines=False)\n",
      " 95%|██████████████████████████████████████████████████████████████████████████████████▍    | 18/19 [04:19<00:15, 15.45s/it]/tmp/ipykernel_443/2200619574.py:66: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  df=pd.read_csv(f,engine=\"python\",error_bad_lines=False)\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████| 19/19 [04:34<00:00, 14.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   source            target  passengers  weight\n",
      "YEAR MONTH                                                     \n",
      "2004 1        aberdeen_sd      jamestown_nd        45.0    25.0\n",
      "     1        aberdeen_sd    minneapolis_mn      2782.0   141.0\n",
      "     1        aberdeen_sd         pierre_sd       505.0    52.0\n",
      "     1        aberdeen_sd    sioux_falls_sd         0.0    35.0\n",
      "     1        aberdeen_sd      watertown_sd       139.0    29.0\n",
      "...                   ...               ...         ...     ...\n",
      "2022 10           yuma_az         laredo_tx       590.0     4.0\n",
      "     10           yuma_az        phoenix_az      6536.0   222.0\n",
      "     10           yuma_az   port_hueneme_ca         0.0     1.0\n",
      "     10     zachar_bay_ak      amook_bay_ak         0.0     1.0\n",
      "     10     zachar_bay_ak  kodiak_island_ak         2.0     3.0\n",
      "\n",
      "[2256649 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import sys\n",
    "import unicodedata\n",
    "\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from datetime import date\n",
    "from glob import glob\n",
    "from collections import defaultdict\n",
    "\n",
    "sys.path.append('src')\n",
    "# from features_extractor import features_extractor\n",
    "\n",
    "def strip_accents(text):\n",
    "    \"\"\"\n",
    "    Strip accents from input String.\n",
    "\n",
    "    :param text: The input string.\n",
    "    :type text: String.\n",
    "\n",
    "    :returns: The processed String.\n",
    "    :rtype: String.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        text = unicode(text, 'utf-8')\n",
    "    except (TypeError, NameError): # unicode is a default on python 3\n",
    "        pass\n",
    "    text = unicodedata.normalize('NFD', text)\n",
    "    text = text.encode('ascii', 'ignore')\n",
    "    text = text.decode(\"utf-8\")\n",
    "    return str(text)\n",
    "\n",
    "def text_to_id(text):\n",
    "    \"\"\"\n",
    "    Convert input text to id.\n",
    "\n",
    "    :param text: The input string.\n",
    "    :type text: String.\n",
    "\n",
    "    :returns: The processed String.\n",
    "    :rtype: String.\n",
    "    \"\"\"\n",
    "    text = strip_accents(text.lower())\n",
    "    text = re.sub(r\"\\d\", \"\", text)\n",
    "    text=re.sub(r\"^\\s+\", \"\", text)\n",
    "    text=re.sub(r\"\\s+$\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\",\"_\", text, flags = re.I)\n",
    "    #text = re.sub('[ ]+', '_', text)\n",
    "    text = re.sub('[^a-zA-Z_-]', '', text)\n",
    "    return text\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    from_file = 'data/raw_usair_data/*.csv'\n",
    "    feature_file = 'data/features/usair_2004_2022.csv'\n",
    "\n",
    "#     try:\n",
    "#         data = pd.read_csv(to_file, sep=';')\n",
    "#         data.set_index(['YEAR', 'MONTH'], inplace=True)\n",
    "#     except:\n",
    "#         print(f'{to_file} not found! Generating graphs from raw')\n",
    "    dfs=[]\n",
    "    for f in tqdm(sorted(glob(from_file))):\n",
    "        df=pd.read_csv(f,engine=\"python\",error_bad_lines=False)\n",
    "        df=df[['YEAR','MONTH','ORIGIN_CITY_NAME','DEST_CITY_NAME','PASSENGERS','DEPARTURES_PERFORMED']]\n",
    "        df=df.rename(index=str, columns={\"ORIGIN_CITY_NAME\": \"source\",\n",
    "                                         \"DEST_CITY_NAME\": \"target\",\n",
    "                                         'PASSENGERS':'passengers',\n",
    "                                         'DEPARTURES_PERFORMED':'weight'})\n",
    "        df['source']=df.apply(lambda row: text_to_id(str(row.source)), axis=1)\n",
    "        df['target']=df.apply(lambda row: text_to_id(str(row.target)), axis=1)\n",
    "        df=df.groupby(['YEAR','MONTH','source','target']).sum()\n",
    "        df=df.reset_index()\n",
    "        dfs.append(df[df.weight !=0 ])\n",
    "    data=pd.concat(dfs, ignore_index=True)\n",
    "    data=data.reset_index().drop(columns='index')\n",
    "    data.set_index(['YEAR', 'MONTH'],inplace=True)\n",
    "    data.sort_index(inplace=True)\n",
    "    # data.to_csv(to_file,sep=';')\n",
    "\n",
    "    data = data[data.source != data.target]\n",
    "    data = data[data.weight!=0]\n",
    "    \n",
    "    print(data)\n",
    "#     year = list(data.index.get_level_values(0).unique())\n",
    "#     month = list(data.index.get_level_values(1).unique())\n",
    "#     graphs_air = []\n",
    "#     date_air = []\n",
    "#     for y in year:\n",
    "#         for m in month:\n",
    "#             if y==2022 and m==12:\n",
    "#                 break\n",
    "#             df = data.loc[y,m]\n",
    "#             date_air.append(date(y,m,1))\n",
    "#             G = nx.from_pandas_edgelist(df, edge_attr=True)\n",
    "#             graphs_air.append(G)\n",
    "#     features = features_extractor(graphs_air, date_air)\n",
    "#     features.to_csv(feature_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5dad776",
   "metadata": {},
   "source": [
    "#### Note *** Missing data -- 11/2022***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "caa09d24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                               | 0/224 [00:00<?, ?it/s]/tmp/ipykernel_443/374198604.py:35: FutureWarning: adjacency_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n",
      "  A = nx.adjacency_matrix(G, nodelist=nodeList, weight = None).todense()\n",
      "/tmp/ipykernel_443/374198604.py:39: FutureWarning: adjacency_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n",
      "  A = nx.adjacency_matrix(G, nodelist=nodeList, weight = None).todense()\n",
      "/tmp/ipykernel_443/374198604.py:43: FutureWarning: adjacency_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n",
      "  A = nx.adjacency_matrix(G, nodelist=nodeList, weight='weight').todense()\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 224/224 [5:52:16<00:00, 94.36s/it]\n"
     ]
    }
   ],
   "source": [
    "year = list(data.index.get_level_values(0).unique())\n",
    "month = list(data.index.get_level_values(1).unique())\n",
    "graphs_air = []\n",
    "date_air = []\n",
    "for y in year:\n",
    "    for m in month:\n",
    "        if y==2022 and m==10:\n",
    "            break\n",
    "        df = data.loc[y,m]\n",
    "        date_air.append(date(y,m,1))\n",
    "        G = nx.from_pandas_edgelist(df, edge_attr=True)\n",
    "        graphs_air.append(G)\n",
    "features = features_extractor(graphs_air, date_air)\n",
    "features.to_csv(feature_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7a58b2",
   "metadata": {},
   "source": [
    "### plot_style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41d8c94a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Edge</th>\n",
       "      <th>Year</th>\n",
       "      <th>Common Neighbor</th>\n",
       "      <th>Weighted Common Neighbor</th>\n",
       "      <th>Salton</th>\n",
       "      <th>Weighted Salton</th>\n",
       "      <th>Sorensen</th>\n",
       "      <th>Weighted Sorensen</th>\n",
       "      <th>Hub Promoted</th>\n",
       "      <th>...</th>\n",
       "      <th>Adamic Adar</th>\n",
       "      <th>Weighted Adamic Adar</th>\n",
       "      <th>Jaccard</th>\n",
       "      <th>Weighted Jaccard</th>\n",
       "      <th>Removed</th>\n",
       "      <th>Gravity</th>\n",
       "      <th>Curr Weight</th>\n",
       "      <th>Next Weight</th>\n",
       "      <th>Curr FWeight</th>\n",
       "      <th>Next FWeight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>('aberdeen_sd', 'jamestown_nd')</td>\n",
       "      <td>2004-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.258199</td>\n",
       "      <td>0.172219</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.139410</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.198025</td>\n",
       "      <td>2.646591</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.074928</td>\n",
       "      <td>False</td>\n",
       "      <td>3.794004e+04</td>\n",
       "      <td>29.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>('aberdeen_sd', 'minneapolis_mn')</td>\n",
       "      <td>2004-01-01</td>\n",
       "      <td>4</td>\n",
       "      <td>99.0</td>\n",
       "      <td>0.143223</td>\n",
       "      <td>0.042340</td>\n",
       "      <td>0.049689</td>\n",
       "      <td>0.010551</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>...</td>\n",
       "      <td>3.190348</td>\n",
       "      <td>79.667360</td>\n",
       "      <td>0.025478</td>\n",
       "      <td>0.005303</td>\n",
       "      <td>False</td>\n",
       "      <td>1.511278e+05</td>\n",
       "      <td>137.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>0.000322</td>\n",
       "      <td>0.000328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>('aberdeen_sd', 'pierre_sd')</td>\n",
       "      <td>2004-01-01</td>\n",
       "      <td>2</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.338062</td>\n",
       "      <td>0.151742</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.141876</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.521541</td>\n",
       "      <td>7.715756</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.076355</td>\n",
       "      <td>False</td>\n",
       "      <td>2.600904e+04</td>\n",
       "      <td>59.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>0.000131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>('aberdeen_sd', 'sioux_falls_sd')</td>\n",
       "      <td>2004-01-01</td>\n",
       "      <td>2</td>\n",
       "      <td>166.0</td>\n",
       "      <td>0.190693</td>\n",
       "      <td>0.322516</td>\n",
       "      <td>0.148148</td>\n",
       "      <td>0.278757</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.711924</td>\n",
       "      <td>50.393390</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>0.161951</td>\n",
       "      <td>False</td>\n",
       "      <td>1.715192e+05</td>\n",
       "      <td>42.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>0.000097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>('aberdeen_sd', 'watertown_sd')</td>\n",
       "      <td>2004-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>53.0</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>0.340191</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.280423</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.198025</td>\n",
       "      <td>5.394975</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.163077</td>\n",
       "      <td>False</td>\n",
       "      <td>1.081186e+05</td>\n",
       "      <td>29.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1308971</th>\n",
       "      <td>1308971</td>\n",
       "      <td>('west_point_ak', 'uganik_ak')</td>\n",
       "      <td>2022-08-01</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>1.010896e+07</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1308972</th>\n",
       "      <td>1308972</td>\n",
       "      <td>('santa_maria_ca', 'santa_ynez_ca')</td>\n",
       "      <td>2022-08-01</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.288675</td>\n",
       "      <td>0.075810</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.022472</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.236177</td>\n",
       "      <td>0.121455</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.011364</td>\n",
       "      <td>True</td>\n",
       "      <td>9.942292e+08</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1308973</th>\n",
       "      <td>1308973</td>\n",
       "      <td>('monroe_nc', 'selinsgrove_pa')</td>\n",
       "      <td>2022-08-01</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>1.010896e+07</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1308974</th>\n",
       "      <td>1308974</td>\n",
       "      <td>('st_michael_ak', 'stebbins_ak')</td>\n",
       "      <td>2022-08-01</td>\n",
       "      <td>3</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.408248</td>\n",
       "      <td>0.476288</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.474074</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.921257</td>\n",
       "      <td>30.367745</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.310680</td>\n",
       "      <td>True</td>\n",
       "      <td>7.013384e+05</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1308975</th>\n",
       "      <td>1308975</td>\n",
       "      <td>('port_alexander_ak', 'port_armstrong_ak')</td>\n",
       "      <td>2022-08-01</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.353553</td>\n",
       "      <td>0.258199</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.339623</td>\n",
       "      <td>0.472116</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.103448</td>\n",
       "      <td>False</td>\n",
       "      <td>1.010896e+07</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1308976 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Unnamed: 0                                        Edge        Year  \\\n",
       "0                 0             ('aberdeen_sd', 'jamestown_nd')  2004-01-01   \n",
       "1                 1           ('aberdeen_sd', 'minneapolis_mn')  2004-01-01   \n",
       "2                 2                ('aberdeen_sd', 'pierre_sd')  2004-01-01   \n",
       "3                 3           ('aberdeen_sd', 'sioux_falls_sd')  2004-01-01   \n",
       "4                 4             ('aberdeen_sd', 'watertown_sd')  2004-01-01   \n",
       "...             ...                                         ...         ...   \n",
       "1308971     1308971              ('west_point_ak', 'uganik_ak')  2022-08-01   \n",
       "1308972     1308972         ('santa_maria_ca', 'santa_ynez_ca')  2022-08-01   \n",
       "1308973     1308973             ('monroe_nc', 'selinsgrove_pa')  2022-08-01   \n",
       "1308974     1308974            ('st_michael_ak', 'stebbins_ak')  2022-08-01   \n",
       "1308975     1308975  ('port_alexander_ak', 'port_armstrong_ak')  2022-08-01   \n",
       "\n",
       "         Common Neighbor  Weighted Common Neighbor    Salton  Weighted Salton  \\\n",
       "0                      1                      26.0  0.258199         0.172219   \n",
       "1                      4                      99.0  0.143223         0.042340   \n",
       "2                      2                      31.0  0.338062         0.151742   \n",
       "3                      2                     166.0  0.190693         0.322516   \n",
       "4                      1                      53.0  0.316228         0.340191   \n",
       "...                  ...                       ...       ...              ...   \n",
       "1308971                0                       0.0  0.000000         0.000000   \n",
       "1308972                1                       1.0  0.288675         0.075810   \n",
       "1308973                0                       0.0  0.000000         0.000000   \n",
       "1308974                3                      64.0  0.408248         0.476288   \n",
       "1308975                1                       3.0  0.353553         0.258199   \n",
       "\n",
       "         Sorensen  Weighted Sorensen  Hub Promoted  ...  Adamic Adar  \\\n",
       "0        0.250000           0.139410      0.333333  ...     0.198025   \n",
       "1        0.049689           0.010551      0.800000  ...     3.190348   \n",
       "2        0.333333           0.141876      0.400000  ...     0.521541   \n",
       "3        0.148148           0.278757      0.400000  ...     0.711924   \n",
       "4        0.285714           0.280423      0.500000  ...     0.198025   \n",
       "...           ...                ...           ...  ...          ...   \n",
       "1308971  0.000000           0.000000      0.000000  ...     0.000000   \n",
       "1308972  0.250000           0.022472      0.500000  ...     0.236177   \n",
       "1308973  0.000000           0.000000      0.000000  ...     0.000000   \n",
       "1308974  0.400000           0.474074      0.500000  ...     0.921257   \n",
       "1308975  0.333333           0.187500      0.500000  ...     0.339623   \n",
       "\n",
       "         Weighted Adamic Adar   Jaccard  Weighted Jaccard  Removed  \\\n",
       "0                    2.646591  0.142857          0.074928    False   \n",
       "1                   79.667360  0.025478          0.005303    False   \n",
       "2                    7.715756  0.200000          0.076355    False   \n",
       "3                   50.393390  0.080000          0.161951    False   \n",
       "4                    5.394975  0.166667          0.163077    False   \n",
       "...                       ...       ...               ...      ...   \n",
       "1308971              0.000000  0.000000          0.000000     True   \n",
       "1308972              0.121455  0.142857          0.011364     True   \n",
       "1308973              0.000000  0.000000          0.000000     True   \n",
       "1308974             30.367745  0.250000          0.310680     True   \n",
       "1308975              0.472116  0.200000          0.103448    False   \n",
       "\n",
       "              Gravity  Curr Weight  Next Weight  Curr FWeight  Next FWeight  \n",
       "0        3.794004e+04         29.0         26.0      0.000068      0.000063  \n",
       "1        1.511278e+05        137.0        135.0      0.000322      0.000328  \n",
       "2        2.600904e+04         59.0         54.0      0.000139      0.000131  \n",
       "3        1.715192e+05         42.0         40.0      0.000099      0.000097  \n",
       "4        1.081186e+05         29.0         29.0      0.000068      0.000070  \n",
       "...               ...          ...          ...           ...           ...  \n",
       "1308971  1.010896e+07          3.0          0.0      0.000008      0.000000  \n",
       "1308972  9.942292e+08          1.0          0.0      0.000003      0.000000  \n",
       "1308973  1.010896e+07          1.0          0.0      0.000003      0.000000  \n",
       "1308974  7.013384e+05         25.0          0.0      0.000068      0.000000  \n",
       "1308975  1.010896e+07          2.0          1.0      0.000005      0.000003  \n",
       "\n",
       "[1308976 rows x 32 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"data/features/usair_2004_2022.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0c2c96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.mlab as ml\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import ticker\n",
    "import matplotlib.ticker as plticker\n",
    "\n",
    "def stdfigsize(scale=1, nx=1, ny=1, ratio=1.3):\n",
    "    \"\"\"\n",
    "    Returns a tuple to be used as figure size.\n",
    "    -------\n",
    "    returns (7*ratio*scale*nx, 7.*scale*ny)\n",
    "    By default: ratio=1.3\n",
    "    If ratio<0 them ratio = golden ratio\n",
    "    \"\"\"\n",
    "    if ratio < 0:\n",
    "        ratio = 1.61803398875\n",
    "    return((7*ratio*scale*nx, 7*scale*ny))\n",
    "\n",
    "def stdrcparams(usetex=False):\n",
    "    \"\"\"\n",
    "    Set several mpl.rcParams and sns.set_style for my taste.\n",
    "    ----\n",
    "    usetex = True\n",
    "    ----\n",
    "    \"\"\"\n",
    "    sns.set_style(\"white\")\n",
    "    sns.set_style({\"xtick.direction\": \"in\",\n",
    "                 \"ytick.direction\": \"in\"})\n",
    "    rcparams = {'text.usetex': usetex,\n",
    "              'font.family': 'sans-serif',\n",
    "              'font.sans-serif': ['Helvetica'],\n",
    "             # 'text.latex.unicode': True,\n",
    "              'text.latex.preamble': [r\"\\usepackage[T1]{fontenc}\",\n",
    "                                      r\"\\usepackage{lmodern}\",\n",
    "                                      r\"\\usepackage{amsmath}\",\n",
    "                                      r\"\\usepackage{mathptmx}\"\n",
    "                                      ],\n",
    "              'axes.labelsize': 30,\n",
    "              'axes.titlesize': 30,\n",
    "              'ytick.right': 'on',\n",
    "              'xtick.top': 'on',\n",
    "              'xtick.labelsize': '25',\n",
    "              'ytick.labelsize': '25',\n",
    "              'axes.linewidth': 1.8,\n",
    "              'xtick.major.width': 1.8,\n",
    "              'xtick.minor.width': 1.8,\n",
    "              'xtick.major.size': 14,\n",
    "              'xtick.minor.size': 7,\n",
    "              'xtick.major.pad': 10,\n",
    "              'xtick.minor.pad': 10,\n",
    "              'ytick.major.width': 1.8,\n",
    "              'ytick.minor.width': 1.8,\n",
    "              'ytick.major.size': 14,\n",
    "              'ytick.minor.size': 7,\n",
    "              'ytick.major.pad': 10,\n",
    "              'ytick.minor.pad': 10,\n",
    "              'axes.labelpad': 15,\n",
    "              'axes.titlepad': 15,\n",
    "              \"xtick.direction\": \"in\",\n",
    "              \"ytick.direction\": \"in\",\n",
    "              'legend.fontsize': 20}\n",
    "    mpl.rcParams.update(rcparams)\n",
    "\n",
    "mpl.rcParams['lines.linewidth'] = 5\n",
    "mpl.rcParams['lines.color'] = '#3690c0'\n",
    "\n",
    "stdrcparams(usetex=True)\n",
    "figsize=stdfigsize(ratio=-1)\n",
    "xs,ys=figsize\n",
    "\n",
    "def custom_frame(ax):\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.get_xaxis().tick_bottom()\n",
    "    ax.get_yaxis().tick_left()\n",
    "    ax.tick_params(axis='x',length=10,direction='out')\n",
    "    ax.tick_params(axis='x',which='minor',direction='out')\n",
    "    ax.tick_params(axis='y',length=10,direction='out')\n",
    "    ax.tick_params(axis='y',which='minor',direction='out')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef322e76",
   "metadata": {},
   "source": [
    "### classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "21b4f1a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_443/3634558621.py:25: DeprecationWarning: Sampling from a set deprecated\n",
      "since Python 3.9 and will be removed in a subsequent version.\n",
      "  edge_train = set(random.sample(edges, int(f_train_e*len(edges))))\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 224/224 [04:12<00:00,  1.13s/it]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 224/224 [26:08<00:00,  7.00s/it]\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'shap'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [98], line 179\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m best_params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    178\u001b[0m     best_params \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 179\u001b[0m \u001b[43mBTF\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m WTF(train, test)\n\u001b[1;32m    181\u001b[0m WWW(train, test)\n",
      "Cell \u001b[0;32mIn [98], line 139\u001b[0m, in \u001b[0;36mBTF\u001b[0;34m(train, test)\u001b[0m\n\u001b[1;32m    137\u001b[0m simultaneous_test(train, test, features, best_params, name \u001b[38;5;241m=\u001b[39m name)\n\u001b[1;32m    138\u001b[0m nonsimultaneous_test(train, test, features, best_params, name \u001b[38;5;241m=\u001b[39m name)\n\u001b[0;32m--> 139\u001b[0m \u001b[43mall_shap_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbest_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [98], line 98\u001b[0m, in \u001b[0;36mall_shap_values\u001b[0;34m(df1, df2, features, best_params, save, name)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mall_shap_values\u001b[39m(df1, df2, features, best_params, save\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 98\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mshap\u001b[39;00m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    100\u001b[0m         name \u001b[38;5;241m=\u001b[39m  \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([w[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m features]) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_SHAP\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'shap'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "import scipy.stats as ss\n",
    "\n",
    "from datetime import date\n",
    "from tqdm import tqdm\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import confusion_matrix,balanced_accuracy_score, mean_squared_error,r2_score,mean_absolute_error\n",
    "\n",
    "sys.path.append('src')\n",
    "# from plot_style import *\n",
    "\n",
    "def get_edge_slice(data, f_train_e=0.7, seed=30):\n",
    "    df = data\n",
    "    edges = set(df.Edge.unique())\n",
    "    random.seed(seed)\n",
    "    edge_train = set(random.sample(edges, int(f_train_e*len(edges))))\n",
    "    edge_test = set([e for e in edges if e not in edge_train])\n",
    "    df_se = df.loc[df['Edge'].isin(edge_train)].drop(columns = ['Edge'])\n",
    "    df_de = df.loc[df['Edge'].isin(edge_test)].drop(columns = ['Edge'])\n",
    "    return(df_se, df_de)\n",
    "\n",
    "def df_to_XY(df, features, target='Removed'):\n",
    "    if 'Year' in df.columns:\n",
    "        df = df.drop(columns = ['Year'])\n",
    "    if \"Edge\" in df.columns:\n",
    "        df = df.drop(columns = ['Edge'])\n",
    "    X = df.loc[:, features].to_numpy()\n",
    "    y = df.loc[:, df.columns == target].to_numpy()\n",
    "    return(X, y)\n",
    "\n",
    "def simultaneous_test(df_se, df_de, features, best_params, save = True, name = None):\n",
    "    if name is None:\n",
    "        name = ''.join([w[0] for w in features]) + '_simultaneous'\n",
    "    else:\n",
    "        name =  name + '_simultaneous'\n",
    "    year_list = list(df_se.Year.unique())\n",
    "    res_df_de = df_de.copy()\n",
    "    res_df_de['simultaneous_pred']= np.nan\n",
    "    res_df_de['simultaneous_null']= np.nan\n",
    "    for year in tqdm(year_list):\n",
    "        X_train,y_train = df_to_XY(df_se[df_se.Year==year],features)\n",
    "        ros = RandomUnderSampler()\n",
    "        X_train,y_train = ros.fit_resample(X_train,y_train)\n",
    "        X_test,y_test = df_to_XY(df_de[df_de.Year==year],features)\n",
    "        y_train_null = y_train.copy()\n",
    "        np.random.shuffle(y_train_null)\n",
    "        model = XGBClassifier(**best_params)\n",
    "        model.fit(X_train, y_train)\n",
    "        model_null = XGBClassifier(**best_params)\n",
    "        model_null.fit(X_train, y_train_null)\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred_null = model_null.predict(X_test)\n",
    "        res_df_de.loc[res_df_de.Year==year, 'simultaneous_pred'] = y_pred\n",
    "        res_df_de.loc[res_df_de.Year==year, 'simultaneous_null'] = y_pred_null\n",
    "    if save:\n",
    "        res_df_de.to_csv('data\\\\results\\\\'+name+'.csv')\n",
    "    return res_df_de\n",
    "\n",
    "def nonsimultaneous_test(df_train, df_test, features, best_params, save=True, name = None):\n",
    "    if name is None:\n",
    "        name =  ''.join([w[0] for w in features]) + '_nonsimultaneous'\n",
    "    else:\n",
    "        name = name + '_nonsimultaneous'\n",
    "    year_list = list(df_test.Year.unique())\n",
    "    preds = []\n",
    "    for year_train in tqdm(year_list):\n",
    "        X_train,y_train = df_to_XY(df_train[df_train.Year==year_train],features)\n",
    "        ros = RandomUnderSampler()\n",
    "        X_train,y_train = ros.fit_resample(X_train,y_train)\n",
    "        y_train_null = y_train.copy()\n",
    "        np.random.shuffle(y_train_null)\n",
    "        model = XGBClassifier(**best_params)\n",
    "        model.fit(X_train, y_train)\n",
    "        model_null = XGBClassifier(**best_params)\n",
    "        model_null.fit(X_train, y_train_null)\n",
    "        i = year_list.index(year_train)\n",
    "        for year_test in year_list[i:]:\n",
    "            X_test, y_test = df_to_XY(df_test[df_test.Year==year_test],features)\n",
    "            y_pred = model.predict(X_test)\n",
    "            y_null = model_null.predict(X_test)\n",
    "            preds.append([year_train ,year_test, y_test,y_pred,y_null])\n",
    "    if save:\n",
    "        import pickle\n",
    "        with open('data\\\\results\\\\'+name+'.pkl', 'wb') as f:\n",
    "            pickle.dump(preds, f)\n",
    "    return preds\n",
    "\n",
    "def all_shap_values(df1, df2, features, best_params, save=True, name = None):\n",
    "    import shap\n",
    "    if name is None:\n",
    "        name =  ''.join([w[0] for w in features]) + '_SHAP'\n",
    "    else:\n",
    "        name =  name + '_SHAP'\n",
    "    def get_temporal_order(shap_list):\n",
    "        importance_array = []\n",
    "        for shap_values in shap_list:\n",
    "            array = -np.abs(shap_values).mean(axis=0)\n",
    "            ranks = ss.rankdata(array)\n",
    "            importance_array.append(ranks)\n",
    "        return(np.array(importance_array))\n",
    "\n",
    "    shap_values_list = []\n",
    "    test_list = []\n",
    "    year_list = []\n",
    "    for i in tqdm(df2.Year.unique()):\n",
    "        X_train,y_train = df_to_XY(df1[ df1.Year == i ].drop(columns = ['Year']),features)\n",
    "        ros = RandomUnderSampler()\n",
    "        X_train,y_train = ros.fit_resample(X_train,y_train)\n",
    "        X_test,y_test = df_to_XY(df2[ df2.Year == i ].drop(columns = ['Year']), features)\n",
    "        model = XGBClassifier(**best_params)\n",
    "        model.fit(X_train, y_train)\n",
    "        explainer = shap.TreeExplainer(model)\n",
    "        shap_values = explainer.shap_values(X_test)\n",
    "        year_list.append(i)\n",
    "        test_list.append(pd.DataFrame(X_test,columns=features))\n",
    "        shap_values_list.append(shap_values)\n",
    "    if save:\n",
    "        import pickle\n",
    "        with open('data\\\\results\\\\'+name+'.pkl', 'wb') as f:\n",
    "            pickle.dump((test_list, year_list, shap_values_list), f)\n",
    "    return (test_list, year_list, shap_values_list)\n",
    "\n",
    "def BTF(train, test):\n",
    "    name = 'Air_Classification_BTF'\n",
    "    features = ['Common Neighbor', 'Salton', 'Jaccard', 'Sorensen', 'Hub Promoted',\n",
    "               'Hub Depressed', 'Leicht Holme Newman', 'Preferential Attachment',\n",
    "               'Adamic Adar', 'Resource Allocation', 'Local Path']\n",
    "    simultaneous_test(train, test, features, best_params, name = name)\n",
    "    nonsimultaneous_test(train, test, features, best_params, name = name)\n",
    "    all_shap_values(train, test, features, best_params, name = name)\n",
    "\n",
    "def WTF(train, test):\n",
    "    name = 'Air_Classification_WTF'\n",
    "    features = []\n",
    "    for c in data.columns:\n",
    "        if  'Weighted' in c:\n",
    "            features.append(c)\n",
    "    simultaneous_test(train, test, features, best_params,name = name)\n",
    "    nonsimultaneous_test(train, test, features, best_params, name = name)\n",
    "    all_shap_values(train, test, features, best_params, name = name)\n",
    "\n",
    "def WWW(train, test):\n",
    "    name = 'Air_Classification_WWW'\n",
    "    features = ['Curr FWeight']\n",
    "    simultaneous_test(train, test, features, best_params, name = name)\n",
    "    nonsimultaneous_test(train, test, features, best_params, name = name)\n",
    "    all_shap_values(train, test, features, best_params, name = name)\n",
    "\n",
    "def BTFW(train, test):\n",
    "    name = 'Air_Classification_BTFW'\n",
    "    features = ['Common Neighbor', 'Salton', 'Jaccard', 'Sorensen', 'Hub Promoted',\n",
    "               'Hub Depressed', 'Leicht Holme Newman', 'Preferential Attachment',\n",
    "               'Adamic Adar', 'Resource Allocation', 'Local Path','Curr FWeight']\n",
    "\n",
    "    simultaneous_test(train, test, features, best_params, name = name)\n",
    "    nonsimultaneous_test(train, test, features, best_params, name = name)\n",
    "    all_shap_values(train, test, features, best_params, name = name)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # to run classification on bus change the data\n",
    "    # data =\n",
    "\n",
    "    global best_params\n",
    "    best_params = None\n",
    "    for data_path in [\"data/features/usair_2004_2022.csv\"]:\n",
    "        data = pd.read_csv(data_path)\n",
    "        train, test = get_edge_slice(data)\n",
    "        if best_params is None:\n",
    "            best_params = {}\n",
    "        BTF(train, test)\n",
    "        WTF(train, test)\n",
    "        WWW(train, test)\n",
    "        BTFW(train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72374b14",
   "metadata": {},
   "source": [
    "### prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959dcb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "\n",
    "import random\n",
    "import math\n",
    "import datetime\n",
    "\n",
    "from datetime import date\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from sklearn.metrics import confusion_matrix,balanced_accuracy_score, \\\n",
    "mean_squared_error,r2_score,mean_absolute_error\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"src/\")\n",
    "from plot_style import *\n",
    "\n",
    "def get_feature_vector(graph):\n",
    "    def local_path(G, nodeList, epsilon = 0.01):\n",
    "        A = nx.adjacency_matrix(G, nodelist=nodeList, weight = None).todense()\n",
    "        return (A**2+epsilon*A**3)\n",
    "\n",
    "    def weighted_local_path(G, nodeList, epsilon = 0.01):\n",
    "        A = nx.adjacency_matrix(G, nodelist=nodeList, weight='weight').todense()\n",
    "        return (A**2+epsilon*A**3)\n",
    "    X = defaultdict(list)\n",
    "    G = graph\n",
    "    Gedges = list(G.edges())\n",
    "    nodeList = list(G.nodes())\n",
    "    nodeIndex = {node: idx for idx,node in enumerate(nodeList)}\n",
    "\n",
    "    Ki = dict(G.degree())\n",
    "    Wi = dict(G.degree(weight='weight'))\n",
    "    LPI = local_path(G, nodeList)\n",
    "    WLPI = weighted_local_path(G, nodeList)\n",
    "    for j, e in enumerate(Gedges):\n",
    "        u, v = e\n",
    "        common_ns = list(nx.common_neighbors(G,u,v))\n",
    "        w_common_ns = sum([min(G[u][z]['weight'], G[v][z]['weight']) for z in common_ns])\n",
    "        union_ns = set(G.neighbors(u))|set(G.neighbors(v))\n",
    "        w_union_ns = Wi[u] + Wi[v]- w_common_ns\n",
    "        if(w_union_ns==0): print(Wi[u] , Wi[v], [min(G[u][z]['weight'], G[v][z]['weight']) for z in common_ns])\n",
    "        X['Edge'].append(e)\n",
    "\n",
    "        X['Common Neighbor'].append(len(common_ns))\n",
    "        X['Weighted Common Neighbor'].append(w_common_ns)\n",
    "\n",
    "        X['Salton'].append(len(common_ns)/math.sqrt(Ki[u]*Ki[v]))\n",
    "        X['Weighted Salton'].append(w_common_ns/math.sqrt(Wi[u]*Wi[v]))\n",
    "\n",
    "        X['Sorensen'].append(2*len(common_ns)/(Ki[u]+Ki[v]))\n",
    "        X['Weighted Sorensen'].append(2*w_common_ns/(Wi[u]+Wi[v]))\n",
    "\n",
    "        X['Hub Promoted'].append(len(common_ns)/min(Ki[u],Ki[v]))\n",
    "        X['Weighted Hub Promoted'].append(w_common_ns/min(Wi[u],Wi[v]))\n",
    "\n",
    "        X['Hub Depressed'].append(len(common_ns)/max(Ki[u],Ki[v]))\n",
    "        X['Weighted Hub Depressed'].append(w_common_ns/max(Wi[u],Wi[v]))\n",
    "\n",
    "        X['Leicht Holme Newman'].append(len(common_ns)/(Ki[u]*Ki[v]))\n",
    "        X['Weighted Leicht Holme Newman'].append(w_common_ns/(Wi[u]*Wi[v]))\n",
    "\n",
    "        X['Preferential Attachment'].append(Ki[u]*Ki[v])\n",
    "        X['Weighted Preferential Attachment'].append(Wi[u]*Wi[v])\n",
    "\n",
    "        X['Local Path'].append(LPI[nodeIndex[u],nodeIndex[v]])\n",
    "        X['Weighted Local Path'].append(WLPI[nodeIndex[u],nodeIndex[v]])\n",
    "        if len(common_ns)>0:\n",
    "            X['Resource Allocation'].append(sum([1/Ki[z] for z in common_ns]))\n",
    "            X['Weighted Resource Allocation'].append(w_common_ns*sum([1/Wi[z] for z in common_ns]))\n",
    "\n",
    "            X['Adamic Adar'].append(sum([1/math.log(Ki[z]) for z in common_ns]))\n",
    "            X['Weighted Adamic Adar'].append(w_common_ns*sum([1/math.log(Wi[z]+1) for z in common_ns]))\n",
    "\n",
    "            X['Jaccard'].append(len(common_ns)/len(union_ns))\n",
    "            X['Weighted Jaccard'].append(w_common_ns/w_union_ns)\n",
    "        else:\n",
    "            X['Resource Allocation'].append(0)\n",
    "            X['Weighted Resource Allocation'].append(0)\n",
    "            X['Adamic Adar'].append(0)\n",
    "            X['Weighted Adamic Adar'].append(0)\n",
    "            X['Jaccard'].append(0)\n",
    "            X['Weighted Jaccard'].append(0)\n",
    "\n",
    "        X['Curr Weight'].append(G[u][v]['weight'])\n",
    "        X['Curr FWeight'].append(G[u][v]['weight']/G.size(weight='weight'))\n",
    "    df = pd.DataFrame(X)\n",
    "    return(df)\n",
    "\n",
    "def get_edge_slice(data, f_train_e=0.7, seed=30):\n",
    "    df = data\n",
    "    edges = set(df.Edge.unique())\n",
    "    random.seed(seed)\n",
    "    edge_train = set(random.sample(edges, int(f_train_e*len(edges))))\n",
    "    edge_test = set([e for e in edges if e not in edge_train])\n",
    "    df_se = df.loc[df['Edge'].isin(edge_train)].drop(columns = ['Edge'])\n",
    "    df_de = df.loc[df['Edge'].isin(edge_test)].drop(columns = ['Edge'])\n",
    "    return(df_se, df_de)\n",
    "\n",
    "def df_to_XY(df, features, target='Removed'):\n",
    "    if 'Year' in df.columns:\n",
    "        df = df.drop(columns = ['Year'])\n",
    "    if \"Edge\" in df.columns:\n",
    "        df = df.drop(columns = ['Edge'])\n",
    "    X = df.loc[:, features].to_numpy()\n",
    "    y = df.loc[:, df.columns == target].to_numpy()\n",
    "    return(X, y, df.loc[:, df.columns == 'Next Weight'].to_numpy())\n",
    "\n",
    "def add_edges(graphs, inp_graph, time_idx):\n",
    "    GI0,GI1 = graphs[time_idx+1],graphs[time_idx]\n",
    "    GI0.add_nodes_from([n for n in GI1 if n not in GI0])\n",
    "    GI1.add_nodes_from([n for n in GI0 if n not in GI1])\n",
    "    added_edges = list(nx.difference(GI0,GI1).edges())\n",
    "    return added_edges\n",
    "\n",
    "def main(year_start):\n",
    "    if year_start.month==1: return\n",
    "    data = pd.read_csv('../data/networks/US_Air_2004_2022.csv', sep=';')\n",
    "    data.set_index(['YEAR', 'MONTH'], inplace=True)\n",
    "    data = data[data.source != data.target]\n",
    "    nodes = set(data.source) & set(data.target)\n",
    "    data = data[data.weight!=0]\n",
    "    year = list(data.index.get_level_values(0).unique())\n",
    "    month = list(data.index.get_level_values(1).unique())\n",
    "    graphs_air = []\n",
    "    air_dates = []\n",
    "    for y in year:\n",
    "        for m in month:\n",
    "            if y==2022 and m==10:\n",
    "                break\n",
    "            df = data.loc[y,m]\n",
    "            air_dates.append(date(y,m,1))\n",
    "            G = nx.from_pandas_edgelist(df, edge_attr=True)\n",
    "            G.add_nodes_from(nodes)\n",
    "            graphs_air.append(G)\n",
    "\n",
    "    data = pd.read_csv('data/features/US_Air_2004_2022.csv')\n",
    "    out = {}\n",
    "    idx = air_dates.index(year_start)\n",
    "    train, test = get_edge_slice(data)\n",
    "    features = ['Common Neighbor', 'Salton', 'Jaccard', 'Sorensen', 'Hub Promoted',\n",
    "           'Hub Depressed', 'Leicht Holme Newman', 'Preferential Attachment',\n",
    "           'Adamic Adar', 'Resource Allocation', 'Local Path']\n",
    "    X_train, y_train, _ = df_to_XY(train[train.Year==str(year_start)],features)\n",
    "    ros = RandomUnderSampler()\n",
    "    X_train,y_train = ros.fit_resample(X_train,y_train)\n",
    "    model_btf = XGBClassifier()\n",
    "    model_btf.fit(X_train, y_train)\n",
    "    model = model_btf\n",
    "    diff_btf = []\n",
    "    graphs_btf = [graphs_air[idx]]\n",
    "    for i in tqdm(range(0,36)):\n",
    "        G = graphs_btf[i].copy()\n",
    "        df = get_feature_vector(G)\n",
    "        edges, X = df['Edge'].to_numpy(),df[features].to_numpy()\n",
    "\n",
    "        GI0,GI1 = graphs_air[idx+i],graphs_air[idx+i+1]\n",
    "        GI0.add_nodes_from([n for n in GI1 if n not in GI0])\n",
    "        GI1.add_nodes_from([n for n in GI0 if n not in GI1])\n",
    "        real_removal = set(nx.difference(GI0,GI1).edges())\n",
    "\n",
    "        pred_prob = model.predict_proba(X).T[0]\n",
    "        added_edges = add_edges(graphs_air, G, idx+i)\n",
    "        N_add = len(added_edges)\n",
    "        for u,v in added_edges:\n",
    "            G.add_edge(u, v, weight=graphs_air[idx+i+1][u][v]['weight'])\n",
    "        N_remove = G.number_of_edges() - graphs_air[idx+i+1].number_of_edges()\n",
    "\n",
    "        removal = zip(edges,pred_prob)\n",
    "        removal = sorted(removal, key = lambda x: x[1])[0:N_remove]\n",
    "        remove_edges = [i for i,_ in removal]\n",
    "        diff_btf.append(len(set(remove_edges)&real_removal)/N_remove)\n",
    "        G.remove_edges_from(remove_edges)\n",
    "        graphs_btf.append(G.copy())\n",
    "    best_params = {'lambda': 0.5650701862593042, 'alpha': 0.0016650896783581535,\n",
    "           'colsample_bytree': 1.0, 'subsample': 0.5, 'learning_rate': 0.009,\n",
    "           'n_estimators': 625, 'objective':'reg:squarederror','max_depth': 5, 'min_child_weight': 6}\n",
    "    features = ['Curr Weight']\n",
    "    X_train, y_train, y_reg = df_to_XY(train[train.Year==str(year_start)],features)\n",
    "    ros = RandomUnderSampler()\n",
    "    X_resample, y_resample = ros.fit_resample(X_train,y_train)\n",
    "    model_www = XGBClassifier()\n",
    "    model_www.fit(X_resample, y_resample)\n",
    "    model_reg = XGBRegressor(**best_params)\n",
    "    model_reg.fit(X_train, y_reg)\n",
    "    model = model_www\n",
    "    diff_www = []\n",
    "    graphs_www = [graphs_air[idx]]\n",
    "    for i in tqdm(range(0,36)):\n",
    "        G = graphs_www[i].copy()\n",
    "        for u,v in G.edges():\n",
    "            G[u][v]['weight'] = model_reg.predict([G[u][v]['weight']])[0]\n",
    "        df = get_feature_vector(G)\n",
    "        edges, X = df['Edge'].to_numpy(),df[features].to_numpy()\n",
    "        pred_prob = model.predict_proba(X).T[0]\n",
    "        GI0,GI1 = graphs_air[idx+i],graphs_air[idx+i+1]\n",
    "        GI0.add_nodes_from([n for n in GI1 if n not in GI0])\n",
    "        GI1.add_nodes_from([n for n in GI0 if n not in GI1])\n",
    "        real_removal = set(nx.difference(GI0,GI1).edges())\n",
    "        added_edges = add_edges(graphs_air, G, idx+i)\n",
    "        N_add = len(added_edges)\n",
    "        for u,v in added_edges:\n",
    "            G.add_edge(u, v, weight=graphs_air[idx+i+1][u][v]['weight'])\n",
    "        N_remove = G.number_of_edges() - graphs_air[idx+i+1].number_of_edges()\n",
    "        removal = zip(edges,pred_prob)\n",
    "        removal = sorted(removal, key = lambda x: x[1])[0:N_remove]\n",
    "        remove_edges = [i for i,_ in removal]\n",
    "        diff_www.append(len(set(remove_edges)&real_removal)/N_remove)\n",
    "        G.remove_edges_from(remove_edges)\n",
    "        graphs_www.append(G.copy())\n",
    "    btf_diff = []\n",
    "    www_diff = []\n",
    "    for i in range(0,36):\n",
    "        G = set(graphs_air[idx+i].edges())\n",
    "        H = set(graphs_btf[i].edges())\n",
    "        btf_diff.append(len(G & H)/len(G))\n",
    "        H = set(graphs_www[i].edges())\n",
    "        www_diff.append(len(G & H)/len(G))\n",
    "    out[year_start]=(diff_btf,diff_www,btf_diff,www_diff)\n",
    "    import pickle\n",
    "    with open(f'data\\\\results\\\\'+ f'{str(year_start)}pred_36' +'.pkl', 'wb') as f:\n",
    "        pickle.dump(out, f)\n",
    "\n",
    "\n",
    "    out = {}\n",
    "    idx = air_dates.index(year_start)\n",
    "    diff_null = []\n",
    "    graphs_null = [graphs_air[idx]]\n",
    "    for i in tqdm(range(0,36)):\n",
    "        G = graphs_null[i].copy()\n",
    "        edges = list(G.edges())\n",
    "        GI0,GI1 = graphs_air[idx+i],graphs_air[idx+i+1]\n",
    "        GI0.add_nodes_from([n for n in GI1 if n not in GI0])\n",
    "        GI1.add_nodes_from([n for n in GI0 if n not in GI1])\n",
    "        real_removal = set(nx.difference(GI0,GI1).edges())\n",
    "        added_edges = add_edges(graphs_air, G, idx+i)\n",
    "        N_add = len(added_edges)\n",
    "        for u,v in added_edges:\n",
    "            G.add_edge(u, v, weight=graphs_air[idx+i+1][u][v]['weight'])\n",
    "        N_remove = G.number_of_edges() - graphs_air[idx+i+1].number_of_edges()\n",
    "        remove_edges = random.sample(edges, N_remove)\n",
    "        diff_null.append(len(set(remove_edges)&real_removal)/N_remove)\n",
    "        G.remove_edges_from(remove_edges)\n",
    "        graphs_null.append(G.copy())\n",
    "    null_diff = []\n",
    "    for i in range(0,36):\n",
    "        G = set(graphs_air[idx+i].edges())\n",
    "        H = set(graphs_null[i].edges())\n",
    "        btf_diff.append(len(G & H)/len(G))\n",
    "    out[year_start]=(diff_null,null_diff)\n",
    "    import pickle\n",
    "    with open(f'data\\\\results\\\\'+ f'{str(year_start)}pred_36_null' +'.pkl', 'wb') as f:\n",
    "        pickle.dump(out, f)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    data = pd.read_csv('data/networks/US_Air_2004_202.csv', sep=';')\n",
    "    data.set_index(['YEAR', 'MONTH'], inplace=True)\n",
    "    data = data[data.source != data.target]\n",
    "    nodes = set(data.source) & set(data.target)\n",
    "    data = data[data.weight!=0]\n",
    "    year = list(data.index.get_level_values(0).unique())\n",
    "    month = list(data.index.get_level_values(1).unique())\n",
    "    graphs_air = []\n",
    "    air_dates = []\n",
    "    for y in year:\n",
    "        for m in month:\n",
    "            if y==2022 and m==10:\n",
    "                break\n",
    "            df = data.loc[y,m]\n",
    "            air_dates.append(date(y,m,1))\n",
    "            G = nx.from_pandas_edgelist(df, edge_attr=True)\n",
    "            G.add_nodes_from(nodes)\n",
    "            graphs_air.append(G)\n",
    "\n",
    "    from joblib import Parallel, delayed\n",
    "    import multiprocessing\n",
    "    num_cores = multiprocessing.cpu_count()\n",
    "    results = Parallel(n_jobs=num_cores)(delayed(main)(year_start) for year_start in air_dates[:-36])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
